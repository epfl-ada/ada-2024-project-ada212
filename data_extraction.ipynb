{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikimapper in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (0.2.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wikimapper"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T10:28:31.494690Z",
     "start_time": "2024-12-14T10:28:29.309197Z"
    }
   },
   "id": "d43e0a56f5f5b2fa"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datapackage in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (1.15.4)\r\n",
      "Requirement already satisfied: six>=1.10 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from datapackage) (1.16.0)\r\n",
      "Requirement already satisfied: click>=6.7 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from datapackage) (8.1.7)\r\n",
      "Requirement already satisfied: chardet>=3.0 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from datapackage) (4.0.0)\r\n",
      "Requirement already satisfied: requests>=2.8 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from datapackage) (2.32.3)\r\n",
      "Requirement already satisfied: jsonschema>=2.5 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from datapackage) (4.19.2)\r\n",
      "Requirement already satisfied: unicodecsv>=0.14 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from datapackage) (0.14.1)\r\n",
      "Requirement already satisfied: jsonpointer>=1.10 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from datapackage) (2.1)\r\n",
      "Requirement already satisfied: tableschema>=1.20.4 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from datapackage) (1.20.11)\r\n",
      "Requirement already satisfied: dataflows-tabulator>=1.29 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from datapackage) (1.54.3)\r\n",
      "Requirement already satisfied: boto3>=1.9 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from dataflows-tabulator>=1.29->datapackage) (1.35.54)\r\n",
      "Requirement already satisfied: ijson>=3.0.3 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from dataflows-tabulator>=1.29->datapackage) (3.3.0)\r\n",
      "Requirement already satisfied: jsonlines>=1.1 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from dataflows-tabulator>=1.29->datapackage) (4.0.0)\r\n",
      "Requirement already satisfied: sqlalchemy>=0.9.6 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from dataflows-tabulator>=1.29->datapackage) (2.0.34)\r\n",
      "Requirement already satisfied: linear-tsv>=1.0 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from dataflows-tabulator>=1.29->datapackage) (1.1.0)\r\n",
      "Requirement already satisfied: xlrd>=1.0 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from dataflows-tabulator>=1.29->datapackage) (2.0.1)\r\n",
      "Requirement already satisfied: openpyxl>=2.6 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from dataflows-tabulator>=1.29->datapackage) (3.1.5)\r\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from jsonschema>=2.5->datapackage) (23.1.0)\r\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from jsonschema>=2.5->datapackage) (2023.7.1)\r\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from jsonschema>=2.5->datapackage) (0.30.2)\r\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from jsonschema>=2.5->datapackage) (0.10.6)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from requests>=2.8->datapackage) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from requests>=2.8->datapackage) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from requests>=2.8->datapackage) (2.2.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from requests>=2.8->datapackage) (2024.8.30)\r\n",
      "Requirement already satisfied: cached-property>=1.5 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from tableschema>=1.20.4->datapackage) (2.0.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.4 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from tableschema>=1.20.4->datapackage) (2.9.0.post0)\r\n",
      "Requirement already satisfied: isodate>=0.5.4 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from tableschema>=1.20.4->datapackage) (0.7.2)\r\n",
      "Requirement already satisfied: rfc3986>=1.1.0 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from tableschema>=1.20.4->datapackage) (2.0.0)\r\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.54 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from boto3>=1.9->dataflows-tabulator>=1.29->datapackage) (1.35.54)\r\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from boto3>=1.9->dataflows-tabulator>=1.29->datapackage) (1.0.1)\r\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from boto3>=1.9->dataflows-tabulator>=1.29->datapackage) (0.10.3)\r\n",
      "Requirement already satisfied: et-xmlfile in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from openpyxl>=2.6->dataflows-tabulator>=1.29->datapackage) (1.1.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from sqlalchemy>=0.9.6->dataflows-tabulator>=1.29->datapackage) (4.11.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datapackage"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T10:28:34.996471Z",
     "start_time": "2024-12-14T10:28:33.248776Z"
    }
   },
   "id": "56d382fa3951a3ab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Extraction Notebook"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce92439e20631187"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "<module 'src.utils' from '/Users/ghaliabennani/Desktop/MA3/ADA/ada-2024-project-ada212/src/utils.py'>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import tarfile\n",
    "from wikimapper import WikiMapper\n",
    "from datapackage import Package\n",
    "import numpy as np\n",
    "import src.utils\n",
    "from src.utils import *\n",
    "import importlib\n",
    "importlib.reload(src.utils)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T10:39:08.783850Z",
     "start_time": "2024-12-14T10:39:08.756224Z"
    }
   },
   "id": "b1b7434725b455bd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CMU Dataset\n",
    "Here we fetch from the CMU website our movie dataset and we store it in the data directory.\n",
    "DELETE THE data DIRECTORY BEFORE RUNNING THE FOLLOWING CELL."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c80ef762d32de562"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "MOVIE_CMU_URL = \"http://www.cs.cmu.edu/~ark/personas/data/MovieSummaries.tar.gz\"\n",
    "response = requests.get(MOVIE_CMU_URL, stream=True)\n",
    "file = tarfile.open(fileobj=response.raw, mode=\"r|gz\")\n",
    "file.extractall(path='.')\n",
    "\n",
    "Path(\"MovieSummaries\").rename(\"data\")\n",
    "data_path = Path(\"data\")\n",
    "\n",
    "for file_name in [\"character.metadata.tsv\", \"name.clusters.txt\", \"plot_summaries.txt\", \"README.txt\", \"tvtropes.clusters.txt\"]:\n",
    "    file_path = data_path / file_name\n",
    "    if file_path.exists():\n",
    "        file_path.unlink()\n",
    "\n",
    "cmu_cols = [\"movie_wikipedia_id\", \"movie_freebase_id\", \"movie_title\", \"movie_release\", \"movie_revenue\", \"movie_runtime\", \"movie_languages\", \"movie_countries\", \"movie_genres\"]\n",
    "cmu_df = (pd.read_csv(\n",
    "    data_path / \"movie.metadata.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=cmu_cols,\n",
    "    usecols=[\"movie_wikipedia_id\", \"movie_title\", \"movie_release\", \"movie_revenue\", \"movie_runtime\",  \"movie_languages\", \"movie_countries\", \"movie_genres\"])\n",
    ".assign(\n",
    "    movie_release=lambda df: df.movie_release.astype(str).str.slice(0, 4).replace(\"nan\", pd.NA).astype(\"Int32\"),\n",
    ")\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T10:44:20.055916Z",
     "start_time": "2024-12-14T10:39:10.178639Z"
    }
   },
   "id": "9202231956b0fae0"
  },
  {
   "cell_type": "markdown",
   "source": [
    " # Wikipedia to Wikidata Ids\n",
    "The CMU dataset uses wikipedia ids. To link movies with books we need the movies' wikidata ids. This is what we do in the following cells"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2cf2d26ecf2388e6"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-14 12:04:16,637 - wikimapper.download - INFO - Downloading [https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-page.sql.gz] to [data/enwiki-latest-page.sql.gz]\r\n",
      "2024-12-14 12:14:31,130 - wikimapper.download - INFO - Downloading [https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-page_props.sql.gz] to [data/enwiki-latest-page_props.sql.gz]\r\n",
      "2024-12-14 12:16:00,082 - wikimapper.download - INFO - Downloading [https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-redirect.sql.gz] to [data/enwiki-latest-redirect.sql.gz]\r\n",
      "2024-12-14 12:16:35,422 - wikimapper.processor - INFO - Creating index for [enwiki-latest] in [data/index_enwiki-latest.db]\r\n",
      "2024-12-14 12:16:35,423 - wikimapper.processor - INFO - Parsing pages dump\r\n",
      "2024-12-14 12:18:50,547 - wikimapper.processor - INFO - Creating database index on 'wikipedia_title'\r\n",
      "2024-12-14 12:19:08,029 - wikimapper.processor - INFO - Parsing page properties dump\r\n",
      "2024-12-14 12:19:54,698 - wikimapper.processor - INFO - Parsing redirects dump\r\n",
      "2024-12-14 12:22:48,254 - wikimapper.processor - INFO - Creating database index on 'wikidata_id'\r\n"
     ]
    }
   ],
   "source": [
    "!wikimapper download enwiki-latest --dir data\n",
    "!wikimapper create enwiki-latest --dumpdir data --target data/index_enwiki-latest.db"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T11:23:02.597725Z",
     "start_time": "2024-12-14T11:04:16.476206Z"
    }
   },
   "id": "89d0c03b9527dea7"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "#we put in our dataset the wikidata id corresponding to the wikipedia id we already have.\n",
    "mapper = WikiMapper(data_path / \"index_enwiki-latest.db\")\n",
    "cmu_df = (cmu_df.assign(\n",
    "    movie_wikidata_id = lambda x: x.movie_wikipedia_id.apply(\n",
    "        lambda wikipedia_id: mapper.wikipedia_id_to_id(wikipedia_id)\n",
    "    )\n",
    ")\n",
    "          .drop(columns=[\"movie_wikipedia_id\"])\n",
    "          )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T11:36:29.728103Z",
     "start_time": "2024-12-14T11:36:24.674700Z"
    }
   },
   "id": "72057078c4f6cdf4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Books-Movies Pairs\n",
    "Now that we have the wikidata ids for the movies in our dataset we can identify the book adaptations in the wikidata database thanks to the SPARQL query in the following cell. We also extract the features of the corresponding books."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "915fdfc039bd24ee"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j1/j4l0ld1s6dz8kzf6ptph9n5w0000gn/T/ipykernel_7317/1133718669.py:59: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  book_release = lambda x: pd.to_datetime(x.pubDateLabel, errors='coerce').dt.year.astype('Int64')\n"
     ]
    }
   ],
   "source": [
    "WIKI_DATA_SERVICE_URL = 'https://query.wikidata.org/sparql'\n",
    "query = '''\n",
    "SELECT DISTINCT ?movie ?book ?bookLabel ?authorLabel ?instanceOfLabel ?countryLabel ?pubDateLabel ?genreLabel ?awardLabel ?seriesLabel ?goodreadsLabel\n",
    "WHERE \n",
    "{\n",
    "  VALUES ?bookType { wd:Q47461344 wd:Q7725634 wd:Q571 wd:Q14406742 wd:Q21198342 wd:Q277759 }\n",
    "  VALUES ?movieType { wd:Q11424 wd:Q506240 }\n",
    "\n",
    "  ?book wdt:P31 ?bookType.\n",
    "  OPTIONAL {?book wdt:P50 ?author}\n",
    "  OPTIONAL {?book wdt:P31 ?instanceOf}\n",
    "  OPTIONAL {?book wdt:P495 ?country}\n",
    "  OPTIONAL {?book wdt:P577 ?pubDate}\n",
    "  OPTIONAL {?book wdt:P136 ?genre}\n",
    "  OPTIONAL {?book wdt:P166 ?award}\n",
    "  OPTIONAL {?book wdt:P179 ?series}\n",
    "  OPTIONAL {?book wdt:P8383 ?goodreads}\n",
    "\n",
    "  ?movie wdt:P31 ?movieType;          \n",
    "         wdt:P144 ?book.\n",
    "\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "'''\n",
    "query_result = requests.get(WIKI_DATA_SERVICE_URL, params = {'format': 'json', 'query': query})\n",
    "wikidata_df =pd.DataFrame(query_result.json()['results']['bindings'])\n",
    "for column in wikidata_df.columns:\n",
    "    wikidata_df[column] = wikidata_df[column].apply(lambda x: x['value'] if isinstance(x, dict) and 'value' in x else x)\n",
    "\n",
    "def get_list(series: pd.Series) -> list:\n",
    "    return list(set(series.dropna().tolist()))\n",
    "\n",
    "def mode(x: pd.Series) -> pd.Series:\n",
    "    modes = x.mode()\n",
    "    if len(modes) > 0:\n",
    "        return modes.iloc[0]\n",
    "    return None\n",
    "\n",
    "categories = {\n",
    "    'fiction': {'novel', 'short novel', 'novella', 'serialized fiction', 'short story', 'war fiction', 'magic realist fiction', 'metafiction', 'science fiction', 'suspense in literature', 'horror novel', 'horror fiction', 'crime fiction', 'psychological thriller', 'speculative/fantastic fiction', 'adventure fiction', 'detective fiction', 'noir fiction', 'political novel', 'vampire fiction', 'dystopian fiction', 'social science fiction', 'techno-thriller', 'thriller', 'fantasy', 'Gothic novel', 'picaresque novel', 'mystery fiction', 'post-apocalyptic fiction', 'philosophical fiction', 'romantic fiction', 'Bildungsroman', 'roman à clef', 'comedy', 'black comedy'},\n",
    "    'non_fiction': {'nonfiction', 'memoir', 'autobiography', 'biographical novel', 'biography', 'essay'},\n",
    "    'children': {'children\\'s literature', 'children\\'s fiction', 'young adult fiction', 'children\\'s novel'},\n",
    "    'historical': {'historical fiction', 'historical novel'},\n",
    "    'drama': {'play', 'drama', 'tragedy'},\n",
    "    'anime': {'adventure anime and manga', 'drama anime and manga'},\n",
    "    'fantasy': {'magic realist fiction', 'fantasy', 'vampire fiction', 'fairy tale'},\n",
    "    'science_fiction': {'science fiction', 'dystopian fiction', 'social science fiction', 'techno-thriller', 'post-apocalyptic fiction'},\n",
    "    'horror': {'horror novel', 'horror fiction'},\n",
    "    'thriller': {'psychological thriller', 'thriller'},\n",
    "    'detective': {'detective fiction', 'noir fiction', 'mystery fiction', 'cloak and dagger novel'},\n",
    "    'satire': {'satire', 'satirical fiction', 'metafiction'},\n",
    "    'comedy': {'comedy', 'black comedy'},\n",
    "}\n",
    "\n",
    "wikidata_df = (wikidata_df\n",
    "               .assign(\n",
    "    movie_wikidata_id = lambda x: x.movie.str.split('/').str[-1],\n",
    "    book_wikidata_id = lambda x: x.book.str.split('/').str[-1],\n",
    "    book_release = lambda x: pd.to_datetime(x.pubDateLabel, errors='coerce').dt.year.astype('Int64')\n",
    ")\n",
    "               .groupby(['movie_wikidata_id', 'book_wikidata_id'])\n",
    "               .agg(\n",
    "    book_title = pd.NamedAgg(column='bookLabel', aggfunc=mode),\n",
    "    book_author = ('authorLabel', 'first'),\n",
    "    book_release = ('book_release', 'first'),\n",
    "    book_country = ('countryLabel', 'first'),\n",
    "    book_goodreads_id = ('goodreadsLabel', 'first'),\n",
    "    series = ('seriesLabel', 'first'),\n",
    "    instance_of = pd.NamedAgg(column='instanceOfLabel', aggfunc=get_list),\n",
    "    genre = pd.NamedAgg(column='genreLabel', aggfunc=get_list),\n",
    "    award = pd.NamedAgg(column='awardLabel', aggfunc=get_list)\n",
    ")\n",
    "               .assign(\n",
    "    book_part_of_series = lambda x: x.series.notnull().astype(int),\n",
    "    literary_work = lambda x: x.instance_of.apply(lambda y: 'literary work' in y).astype(int),\n",
    "    written_work = lambda x: x.instance_of.apply(lambda y: 'written work' in y).astype(int),\n",
    "    comic_book_seris = lambda x: x.instance_of.apply(lambda y: 'comic book series' in y).astype(int),\n",
    "    book_series = lambda x: x.instance_of.apply(lambda y: 'book series' in y).astype(int),\n",
    "    manga_series = lambda x: x.instance_of.apply(lambda y: 'manga series' in y).astype(int),\n",
    "    book_fiction = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['fiction'])) > 0).astype(int),\n",
    "    book_non_fiction = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['non_fiction'])) > 0).astype(int),\n",
    "    book_children = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['children'])) > 0).astype(int),\n",
    "    book_historical = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['historical'])) > 0).astype(int),\n",
    "    book_drama = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['drama'])) > 0).astype(int),\n",
    "    book_anime = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['anime'])) > 0).astype(int),\n",
    "    book_fantasy = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['fantasy'])) > 0).astype(int),\n",
    "    book_science_fiction = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['science_fiction'])) > 0).astype(int),\n",
    "    book_horror = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['horror'])) > 0).astype(int),\n",
    "    book_thriller = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['thriller'])) > 0).astype(int),\n",
    "    book_detective = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['detective'])) > 0).astype(int),\n",
    "    book_satire = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['satire'])) > 0).astype(int),\n",
    "    book_comedy = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['comedy'])) > 0).astype(int),\n",
    "    book_won_price = lambda x: x.award.apply(lambda y: len(y) > 0).astype(int),\n",
    ")\n",
    "               .drop(['instance_of', 'genre', 'award', 'series'], axis=1)\n",
    "               .reset_index()\n",
    "               )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T11:47:09.482779Z",
     "start_time": "2024-12-14T11:46:48.681755Z"
    }
   },
   "id": "106bc04b9edccc55"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Book Dataset\n",
    "We create a book dataset still using the Wikidata Query Service."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16e9a41863c02a12"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import time\n",
    "WIKI_DATA_SERVICE_URL = 'https://query.wikidata.org/sparql'\n",
    "query = '''\n",
    "SELECT DISTINCT ?book\n",
    "WHERE \n",
    "{\n",
    "  VALUES ?bookType { wd:Q47461344 wd:Q7725634 wd:Q571 wd:Q14406742 wd:Q21198342 wd:Q277759 }\n",
    "  ?book wdt:P31 ?bookType.\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "'''\n",
    "query_result = requests.get(WIKI_DATA_SERVICE_URL, params = {'format': 'json', 'query': query})\n",
    "book_df = pd.DataFrame(query_result.json()['results']['bindings'])\n",
    "for column in book_df.columns:\n",
    "    book_df[column] = book_df[column].apply(lambda x: x['value'] if isinstance(x, dict) and 'value' in x else x)\n",
    "book_ids = book_df.book.str.split('/').str[-1].tolist()\n",
    "\n",
    "def get_book_wikidata(ids, retries=5, backoff_factor=1):\n",
    "    book_ids = \" \".join([f\"wd:{id}\" for id in ids])\n",
    "    query = (\n",
    "            'SELECT DISTINCT ?movie ?book ?bookLabel ?authorLabel ?instanceOfLabel ?countryLabel '\n",
    "            '?pubDateLabel ?genreLabel ?awardLabel ?seriesLabel '\n",
    "            'WHERE { '\n",
    "            'VALUES ?book {' + book_ids + '} '\n",
    "                                          'OPTIONAL { ?book wdt:P31 ?instanceOf. } '\n",
    "                                          'OPTIONAL { ?book wdt:P136 ?genre. } '\n",
    "                                          'OPTIONAL { ?book wdt:P495 ?country. } '\n",
    "                                          'OPTIONAL { ?book wdt:P577 ?pubDate. } '\n",
    "                                          'OPTIONAL { ?book wdt:P166 ?award. } '\n",
    "                                          'OPTIONAL { ?book wdt:P179 ?series. } '\n",
    "                                          'OPTIONAL { ?book wdt:P50 ?author. } '\n",
    "                                          'OPTIONAL { ?movie wdt:P144 ?book. } '\n",
    "                                          'SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". } '\n",
    "                                          '}'\n",
    "    )\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        query_result = requests.get(WIKI_DATA_SERVICE_URL, params={'format': 'json', 'query': query})\n",
    "\n",
    "        if query_result.status_code == 200:\n",
    "            try:\n",
    "                json_data = query_result.json()\n",
    "                df = pd.DataFrame(json_data['results']['bindings'])\n",
    "                for column in df.columns:\n",
    "                    df[column] = df[column].apply(lambda x: x.get('value') if isinstance(x, dict) else x)\n",
    "                return df\n",
    "            except ValueError as e:\n",
    "                print(\"Error decoding JSON:\", e)\n",
    "                return pd.DataFrame()  # Return an empty DataFrame if JSON error\n",
    "\n",
    "        elif query_result.status_code == 429:\n",
    "            # Exponential backoff\n",
    "            wait_time = backoff_factor * (2 ** attempt)\n",
    "            time.sleep(wait_time)\n",
    "        else:\n",
    "            print(f\"Request failed with status code {query_result.status_code}\")\n",
    "            return pd.DataFrame()  # Return empty DataFrame if other errors\n",
    "\n",
    "    print(\"Max retries reached.\")\n",
    "    return pd.DataFrame()  # Return an empty DataFrame in case of JSON error\n",
    "\n",
    "book_df_list = []\n",
    "for i in range(0, len(book_ids), 100):\n",
    "    book_df_list.append(get_book_wikidata(book_ids[i:i+100]))\n",
    "book_df = pd.concat(book_df_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T13:57:11.293713Z",
     "start_time": "2024-12-14T11:47:12.108941Z"
    }
   },
   "id": "e7edff5ac03810a1"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "book_df = (book_df\n",
    "           .assign(\n",
    "    book_wikidata_id = lambda x: x.book.str.split('/').str[-1],\n",
    "    book_release = lambda x: pd.to_datetime(x.pubDateLabel, errors='coerce').dt.year.astype('Int64')\n",
    ")\n",
    "           .groupby(['book_wikidata_id'])\n",
    "           .agg(\n",
    "    book_title = pd.NamedAgg(column='bookLabel', aggfunc=mode),\n",
    "    book_author = ('authorLabel', 'first'),\n",
    "    book_release = ('book_release', 'first'),\n",
    "    book_country = ('countryLabel', 'first'),\n",
    "    series = ('seriesLabel', 'first'),\n",
    "    instance_of = pd.NamedAgg(column='instanceOfLabel', aggfunc=get_list),\n",
    "    genre = pd.NamedAgg(column='genreLabel', aggfunc=get_list),\n",
    "    award = pd.NamedAgg(column='awardLabel', aggfunc=get_list)\n",
    ")\n",
    "           .assign(\n",
    "    book_part_of_series = lambda x: x.series.notnull().astype(int),\n",
    "    literary_work = lambda x: x.instance_of.apply(lambda y: 'literary work' in y).astype(int),\n",
    "    written_work = lambda x: x.instance_of.apply(lambda y: 'written work' in y).astype(int),\n",
    "    comic_book_seris = lambda x: x.instance_of.apply(lambda y: 'comic book series' in y).astype(int),\n",
    "    book_series = lambda x: x.instance_of.apply(lambda y: 'book series' in y).astype(int),\n",
    "    manga_series = lambda x: x.instance_of.apply(lambda y: 'manga series' in y).astype(int),\n",
    "    book_fiction = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['fiction'])) > 0).astype(int),\n",
    "    book_non_fiction = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['non_fiction'])) > 0).astype(int),\n",
    "    book_children = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['children'])) > 0).astype(int),\n",
    "    book_historical = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['historical'])) > 0).astype(int),\n",
    "    book_drama = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['drama'])) > 0).astype(int),\n",
    "    book_anime = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['anime'])) > 0).astype(int),\n",
    "    book_fantasy = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['fantasy'])) > 0).astype(int),\n",
    "    book_science_fiction = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['science_fiction'])) > 0).astype(int),\n",
    "    book_horror = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['horror'])) > 0).astype(int),\n",
    "    book_thriller = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['thriller'])) > 0).astype(int),\n",
    "    book_detective = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['detective'])) > 0).astype(int),\n",
    "    book_satire = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['satire'])) > 0).astype(int),\n",
    "    book_comedy = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['comedy'])) > 0).astype(int),\n",
    "    book_won_price = lambda x: x.award.apply(lambda y: len(y) > 0).astype(int),\n",
    ")\n",
    "           .drop(['instance_of', 'genre', 'award', 'series'], axis=1)\n",
    "           .reset_index()\n",
    "           )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T13:59:44.650981Z",
     "start_time": "2024-12-14T13:59:01.831996Z"
    }
   },
   "id": "c08f8e0289c9f345"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Goodreads dataset\n",
    "We add some elements to our book dataset, we use a Kaggle dataset curated from Goodreads. We also take care of cleaning the format of the author name and the title."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d89fb3e3cfd1f2c"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: /Users/ghaliabennani/.kaggle: File exists\r\n",
      "mv: kaggle.json: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "! mkdir ~/.kaggle\n",
    "! mv kaggle.json ~/.kaggle/\n",
    "! chmod 600 ~/.kaggle/kaggle.json"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T13:59:48.497440Z",
     "start_time": "2024-12-14T13:59:48.100355Z"
    }
   },
   "id": "a19589919eedde50"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (1.6.17)\r\n",
      "Requirement already satisfied: six>=1.10 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from kaggle) (1.16.0)\r\n",
      "Requirement already satisfied: certifi>=2023.7.22 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from kaggle) (2024.8.30)\r\n",
      "Requirement already satisfied: python-dateutil in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from kaggle) (2.9.0.post0)\r\n",
      "Requirement already satisfied: requests in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from kaggle) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from kaggle) (4.66.5)\r\n",
      "Requirement already satisfied: python-slugify in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from kaggle) (5.0.2)\r\n",
      "Requirement already satisfied: urllib3 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from kaggle) (2.2.2)\r\n",
      "Requirement already satisfied: bleach in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from kaggle) (4.1.0)\r\n",
      "Requirement already satisfied: packaging in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from bleach->kaggle) (24.1)\r\n",
      "Requirement already satisfied: webencodings in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from bleach->kaggle) (0.5.1)\r\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from python-slugify->kaggle) (1.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from requests->kaggle) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ghaliabennani/anaconda3/lib/python3.11/site-packages (from requests->kaggle) (3.7)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T13:59:52.058571Z",
     "start_time": "2024-12-14T13:59:49.961077Z"
    }
   },
   "id": "b4e6443d8e114850"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/bahramjannesarr/goodreads-book-datasets-10m\r\n",
      "License(s): CC0-1.0\r\n",
      "Downloading goodreads-book-datasets-10m.zip to /Users/ghaliabennani/Desktop/MA3/ADA/ada-2024-project-ada212\r\n",
      "100%|███████████████████████████████████████▉| 460M/460M [00:22<00:00, 30.6MB/s]\r\n",
      "100%|████████████████████████████████████████| 460M/460M [00:22<00:00, 21.1MB/s]\r\n",
      "Archive:  goodreads-book-datasets-10m.zip\r\n",
      "  inflating: book1-100k.csv          \r\n",
      "  inflating: book1000k-1100k.csv     \r\n",
      "  inflating: book100k-200k.csv       \r\n",
      "  inflating: book1100k-1200k.csv     \r\n",
      "  inflating: book1200k-1300k.csv     \r\n",
      "  inflating: book1300k-1400k.csv     \r\n",
      "  inflating: book1400k-1500k.csv     \r\n",
      "  inflating: book1500k-1600k.csv     \r\n",
      "  inflating: book1600k-1700k.csv     \r\n",
      "  inflating: book1700k-1800k.csv     \r\n",
      "  inflating: book1800k-1900k.csv     \r\n",
      "  inflating: book1900k-2000k.csv     \r\n",
      "  inflating: book2000k-3000k.csv     \r\n",
      "  inflating: book200k-300k.csv       \r\n",
      "  inflating: book3000k-4000k.csv     \r\n",
      "  inflating: book300k-400k.csv       \r\n",
      "  inflating: book4000k-5000k.csv     \r\n",
      "  inflating: book400k-500k.csv       \r\n",
      "  inflating: book500k-600k.csv       \r\n",
      "  inflating: book600k-700k.csv       \r\n",
      "  inflating: book700k-800k.csv       \r\n",
      "  inflating: book800k-900k.csv       \r\n",
      "  inflating: book900k-1000k.csv      \r\n",
      "  inflating: user_rating_0_to_1000.csv  \r\n",
      "  inflating: user_rating_1000_to_2000.csv  \r\n",
      "  inflating: user_rating_2000_to_3000.csv  \r\n",
      "  inflating: user_rating_3000_to_4000.csv  \r\n",
      "  inflating: user_rating_4000_to_5000.csv  \r\n",
      "  inflating: user_rating_5000_to_6000.csv  \r\n",
      "  inflating: user_rating_6000_to_11000.csv  \r\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d bahramjannesarr/goodreads-book-datasets-10m\n",
    "!unzip goodreads-book-datasets-10m.zip\n",
    "!rm goodreads-book-datasets-10m.zip\n",
    "!rm user_rating_*.csv\n",
    "!mkdir data/goodreads"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T14:00:27.614072Z",
     "start_time": "2024-12-14T13:59:54.443142Z"
    }
   },
   "id": "60ebc2554c7fa4b1"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "\n",
    "!mv book*.csv data/goodreads/"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T14:00:45.121391Z",
     "start_time": "2024-12-14T14:00:44.989933Z"
    }
   },
   "id": "3655da3bdf2e4df6"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def clean_title(title_series: pd.Series) -> pd.Series:\n",
    "    return (title_series\n",
    "            .str.split('(').str[0]\n",
    "            .str.split(':').str[0]\n",
    "            .str.lower()\n",
    "            .str.replace('and', '&')\n",
    "            .str.replace('.', '')\n",
    "            .str.replace(\"'\", '')\n",
    "            .str.replace('-', ' ')\n",
    "            .str.replace(r'\\s+', ' ', regex=True)\n",
    "            .str.strip()\n",
    "            )\n",
    "\n",
    "def clean_author(author_series: pd.Series) -> pd.Series:\n",
    "    initial_letter = (author_series\n",
    "                      .str.strip()\n",
    "                      .str[0]\n",
    "                      .str.lower())\n",
    "    last_name = (author_series\n",
    "                 .str.split(r\"(\\s|-|')\", regex=True)\n",
    "                 .str[-1]\n",
    "                 .str.replace('.', '')\n",
    "                 .str.replace(\"'\", '')\n",
    "                 .str.replace(r'\\s+', ' ', regex=True)\n",
    "                 .str.strip()\n",
    "                 .str.lower()\n",
    "                 )\n",
    "    return initial_letter + \" \" + last_name"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T14:00:46.879683Z",
     "start_time": "2024-12-14T14:00:46.867573Z"
    }
   },
   "id": "1ccd4eeb5ab2ed32"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "wikidata_df = wikidata_df.assign(\n",
    "    join_title = lambda x: clean_title(x.book_title),\n",
    "    join_author = lambda x: clean_author(x.book_author),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T14:00:49.022819Z",
     "start_time": "2024-12-14T14:00:48.684570Z"
    }
   },
   "id": "89ec3dca62aa576c"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "book_df = book_df.assign(\n",
    "    join_title = lambda x: clean_title(x.book_title),\n",
    "    join_author = lambda x: clean_author(x.book_author),\n",
    ")   \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T14:00:52.734273Z",
     "start_time": "2024-12-14T14:00:49.692530Z"
    }
   },
   "id": "69cf8b4dfe7036b9"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for file in data_path.glob('goodreads/book*.csv'):\n",
    "    try:\n",
    "        df = (pd.read_csv(\n",
    "            file,\n",
    "            usecols=['Name', 'Authors', 'Publisher', 'pagesNumber', 'Rating', 'RatingDistTotal']\n",
    "        )\n",
    "              .rename(columns={'pagesNumber': 'book_pages', 'RatingDistTotal': 'book_ratings_count', 'Rating': 'book_rating',\n",
    "                               'Publisher': 'book_publisher', 'Authors': 'book_author',\n",
    "                               'Name': 'book_title'})\n",
    "              )\n",
    "        df_list.append(df)\n",
    "    except:\n",
    "        df = (pd.read_csv(\n",
    "            file,\n",
    "            usecols=['Name', 'Authors', 'Publisher', 'PagesNumber', 'Rating', 'RatingDistTotal']\n",
    "        )\n",
    "              .rename(columns={'PagesNumber': 'book_pages', 'RatingDistTotal': 'book_ratings_count', 'Rating': 'book_rating',\n",
    "                               'Publisher': 'book_publisher', 'Authors': 'book_author',\n",
    "                               'Name': 'book_title'})\n",
    "              )\n",
    "        df_list.append(df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T14:01:09.913953Z",
     "start_time": "2024-12-14T14:01:05.365232Z"
    }
   },
   "id": "13eaf547017d414c"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "goodreads_df = (pd.concat(df_list, ignore_index=True)\n",
    "                .assign(\n",
    "    book_rating = lambda x: x.book_rating.replace(0, np.nan),\n",
    "    book_pages = lambda x: x.book_pages.replace(0, np.nan).astype('Int64'),\n",
    "    book_ratings_count = lambda x: x.book_ratings_count.str.split(':').str[-1].astype('Int64'),\n",
    "    join_title = lambda x: clean_title(x.book_title),\n",
    "    join_author = lambda x: clean_author(x.book_author),\n",
    ")\n",
    "                .merge(\n",
    "    wikidata_df.loc[:, ['join_title', 'join_author']].drop_duplicates(),\n",
    "    on=['join_title', 'join_author'],\n",
    "    how='inner'\n",
    ")\n",
    "                .drop_duplicates(subset=['join_title', 'join_author'])\n",
    "                .drop(['book_title', 'book_author'], axis=1)\n",
    "                )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T14:01:31.027378Z",
     "start_time": "2024-12-14T14:01:18.899262Z"
    }
   },
   "id": "4b15172c98cac9d1"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "goodreads_book_df = (pd.concat(df_list, ignore_index=True)\n",
    "                     .assign(\n",
    "    book_rating = lambda x: x.book_rating.replace(0, np.nan),\n",
    "    book_pages = lambda x: x.book_pages.replace(0, np.nan).astype('Int64'),\n",
    "    book_ratings_count = lambda x: x.book_ratings_count.str.split(':').str[-1].astype('Int64'),\n",
    "    join_title = lambda x: clean_title(x.book_title),\n",
    "    join_author = lambda x: clean_author(x.book_author),\n",
    ")\n",
    "                     .merge(\n",
    "    book_df.loc[:, ['join_title', 'join_author']].drop_duplicates(),\n",
    "    on=['join_title', 'join_author'],\n",
    "    how='inner'\n",
    ")\n",
    "                     .drop_duplicates(subset=['join_title', 'join_author'])\n",
    "                     .drop(['book_title', 'book_author'], axis=1)\n",
    "                     )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T14:01:47.823751Z",
     "start_time": "2024-12-14T14:01:34.375527Z"
    }
   },
   "id": "bf285a04902791f2"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "(book_df\n",
    " .merge(goodreads_book_df, on=['join_title', 'join_author'], how='inner')\n",
    " .drop(columns=['join_title', 'join_author'])\n",
    " .to_csv(\"data/book.csv\", index=False)\n",
    " )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T14:02:22.784705Z",
     "start_time": "2024-12-14T14:02:22.105438Z"
    }
   },
   "id": "43db832243309efa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# More Revenues infomations with TMDB\n",
    "We have seen that there are a lot of missing values when it comes to movie revenues so we fetch some data from The Movie DataBase."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81af13f632d5d4db"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset\r\n",
      "License(s): CC0-1.0\r\n",
      "Downloading the-movies-dataset.zip to /Users/ghaliabennani/Desktop/MA3/ADA/ada-2024-project-ada212\r\n",
      "100%|███████████████████████████████████████▊| 227M/228M [00:09<00:00, 33.0MB/s]\r\n",
      "100%|████████████████████████████████████████| 228M/228M [00:09<00:00, 25.1MB/s]\r\n",
      "Archive:  the-movies-dataset.zip\r\n",
      "  inflating: credits.csv             \r\n",
      "  inflating: keywords.csv            \r\n",
      "  inflating: links.csv               \r\n",
      "  inflating: links_small.csv         \r\n",
      "  inflating: movies_metadata.csv     \r\n",
      "  inflating: ratings.csv             \r\n",
      "  inflating: ratings_small.csv       \r\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d rounakbanik/the-movies-dataset\n",
    "!unzip the-movies-dataset.zip\n",
    "!rm the-movies-dataset.zip\n",
    "!mv movies_metadata.csv data/\n",
    "!rm credits.csv\n",
    "!rm keywords.csv\n",
    "!rm links.csv\n",
    "!rm links_small.csv\n",
    "!rm ratings.csv\n",
    "!rm ratings_small.csv"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T14:02:47.756985Z",
     "start_time": "2024-12-14T14:02:29.669840Z"
    }
   },
   "id": "783fc6f8d82a8c36"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j1/j4l0ld1s6dz8kzf6ptph9n5w0000gn/T/ipykernel_7317/1954682421.py:5: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tmdb_df = (pd.read_csv(\"data/movies_metadata.csv\")\n"
     ]
    }
   ],
   "source": [
    "def replace_jpg(x):\n",
    "    return np.nan if isinstance(x, str) and x.endswith('.jpg') else x\n",
    "\n",
    "\n",
    "tmdb_df = (pd.read_csv(\"data/movies_metadata.csv\")\n",
    "           .assign(\n",
    "    movie_budget = lambda df: df.budget.apply(replace_jpg).astype(\"Int64\").replace(0, pd.NA),\n",
    "    movie_revenue_tmdb = lambda df: df.revenue.replace(0.0, pd.NA).astype(\"Int64\")\n",
    ")\n",
    "           .loc[:, ['imdb_id', 'movie_budget', 'movie_revenue_tmdb']]\n",
    "           .drop_duplicates(subset=['imdb_id'])\n",
    "           )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T14:02:56.411749Z",
     "start_time": "2024-12-14T14:02:56.034641Z"
    }
   },
   "id": "17d6d002288d488b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Making Revenues and Budgets comparable\n",
    "A way to normalize the revenues and budget data and to make the values comparable across time we we adjust for inflation using the US Consumer Price."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "accaa1d60eabd741"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "   year  inflation_adjustment\n0  2014              1.000000\n1  2013              1.003720\n2  2012              1.018793\n3  2011              1.036531\n4  2010              1.067237",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>inflation_adjustment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2014</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2013</td>\n      <td>1.003720</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2012</td>\n      <td>1.018793</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2011</td>\n      <td>1.036531</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2010</td>\n      <td>1.067237</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "cpi_df = (pd.read_csv(\"data_utils/flat-ui__data-Thu Nov 14 2024.csv\")\n",
    "          .rename(columns={'Date': 'date', 'Index': 'cpi', 'Inflation': '_'})\n",
    "          .assign(\n",
    "    year = lambda df: df.date.astype(str).str.slice(0, 4).astype(\"Int32\"),\n",
    "    inflation_adjustment = lambda df: (df.cpi.iloc[0] / df.cpi).astype(float)\n",
    ")\n",
    "         .drop(columns=['date', 'cpi', '_'])\n",
    "         .drop_duplicates(subset=['year'])\n",
    "         .reset_index(drop=True)\n",
    "         .assign(inflation_adjustment = lambda x: x.inflation_adjustment.astype(float))\n",
    "          )\n",
    "cpi_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T14:02:58.743100Z",
     "start_time": "2024-12-14T14:02:58.722287Z"
    }
   },
   "id": "9aba49707867388f"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "cpi_df.to_csv(\"data/inflation_adjustment.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T14:03:00.114Z",
     "start_time": "2024-12-14T14:03:00.103213Z"
    }
   },
   "id": "5ee42d87b24541dc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# IMDb ratings\n",
    "We extract the IMDb ratings for movies to know user's opinions on it. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b3d6990174ec84d"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
      "100 7421k  100 7421k    0     0   9.8M      0 --:--:-- --:--:-- --:--:--  9.8M\r\n",
      "rm: title.ratings.tsv.gz: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!curl -o title.ratings.tsv.gz https://datasets.imdbws.com/title.ratings.tsv.gz\n",
    "!gunzip title.ratings.tsv.gz\n",
    "!mv title.ratings.tsv data/\n",
    "!rm title.ratings.tsv.gz"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T14:03:03.255390Z",
     "start_time": "2024-12-14T14:03:01.837979Z"
    }
   },
   "id": "4e98315786b1b06b"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "imdb_df = (pd.read_csv(\"data/title.ratings.tsv\", sep='\\t')\n",
    ".rename(columns={\n",
    "    'tconst': 'imdb_id',\n",
    "    'averageRating': 'imdb_rating',\n",
    "    'numVotes': 'imdb_total_votes'})\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T14:03:05.045417Z",
     "start_time": "2024-12-14T14:03:04.598909Z"
    }
   },
   "id": "5b4b775e4ee2918b"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT ?movie ?IMDB_ID\n",
    "WHERE\n",
    "{\n",
    "VALUES ?movieType { wd:Q11424 wd:Q506240 }\n",
    "?movie wdt:P31 ?movieType.\n",
    "?movie wdt:P345 ?IMDB_ID.\n",
    "\n",
    "SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "'''\n",
    "query_result = requests.get(WIKI_DATA_SERVICE_URL, params = {'format': 'json', 'query': query})\n",
    "wikidata_imdb_df =pd.DataFrame(query_result.json()['results']['bindings'])\n",
    "for column in wikidata_imdb_df.columns:\n",
    "    wikidata_imdb_df[column] = wikidata_imdb_df[column].apply(lambda x: x['value'] if isinstance(x, dict) and 'value' in x else x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T14:03:21.496649Z",
     "start_time": "2024-12-14T14:03:09.418590Z"
    }
   },
   "id": "9070cdf500109775"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "wikidata_imdb_df = (wikidata_imdb_df\n",
    "                    .assign(\n",
    "    movie_wikidata_id = lambda x: x.movie.str.split('/').str[-1],\n",
    "    imdb_id = lambda x: x.IMDB_ID\n",
    ")\n",
    "                    .loc[:, ['movie_wikidata_id', 'imdb_id']]\n",
    "                    )\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T14:03:23.684826Z",
     "start_time": "2024-12-14T14:03:23.227047Z"
    }
   },
   "id": "293055a387f5ff89"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Merging the datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10e42a7da83c5d0"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "book_adaptation_df = (cmu_df\n",
    "                      .merge(wikidata_imdb_df, on='movie_wikidata_id', how='left')\n",
    "                      .merge(imdb_df, on='imdb_id', how='left')\n",
    "                      .merge(tmdb_df, on='imdb_id', how='left')\n",
    "                      .merge(wikidata_df, on='movie_wikidata_id', how='left')\n",
    "                      .merge(goodreads_df, on=['join_title', 'join_author'], how='left')\n",
    "                      .merge(cpi_df, left_on='movie_release', right_on='year', how='left')\n",
    "                      .assign(\n",
    "    movie_budget = lambda x: x.movie_budget.astype(float),\n",
    "    movie_revenue = lambda x: x.movie_revenue.fillna(x.movie_revenue_tmdb).astype(float),\n",
    "    movie_is_adaptation = lambda x: x.book_wikidata_id.notna()\n",
    ")\n",
    "                      .assign(\n",
    "    movie_budget = lambda df: df.movie_budget * df.inflation_adjustment,\n",
    "    movie_revenue = lambda df: df.movie_revenue * df.inflation_adjustment\n",
    ")\n",
    "                      .drop(columns=['imdb_id', 'movie_revenue_tmdb', 'book_goodreads_id',\n",
    "                                     'year', 'inflation_adjustment', 'join_title', 'join_author'])\n",
    "                      )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T14:03:25.997212Z",
     "start_time": "2024-12-14T14:03:25.271592Z"
    }
   },
   "id": "4a280dcd98ce8803"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "book_adaptation_df.to_csv(\"data/book_adaptation.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T14:03:29.004363Z",
     "start_time": "2024-12-14T14:03:28.358933Z"
    }
   },
   "id": "cba945677032b5be"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dealing with missing revenue and budget data\n",
    "During the data cleaning process, we found out that there are too much missing values for the movies revenues and budget so we decided to extract some more data about it from the wikidata database."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a33448ca848a646"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total NaN values in movie_wikidata_id: 4288\n",
      "Total unique movie_wikidata_id values: 76544\n",
      "Total rows in original DataFrame: 82053\n"
     ]
    }
   ],
   "source": [
    "# Count of NaNs in movie_wikidata_id\n",
    "print(f\"Total NaN values in movie_wikidata_id: {book_adaptation_df['movie_wikidata_id'].isna().sum()}\")\n",
    "\n",
    "# Check for unique values and potential duplicates\n",
    "unique_ids = book_adaptation_df['movie_wikidata_id'].nunique()\n",
    "print(f\"Total unique movie_wikidata_id values: {unique_ids}\")\n",
    "print(f\"Total rows in original DataFrame: {len(book_adaptation_df)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T14:03:30.591005Z",
     "start_time": "2024-12-14T14:03:30.579035Z"
    }
   },
   "id": "6d14390b56cf685f"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "WIKI_DATA_SERVICE_URL = 'https://query.wikidata.org/sparql'\n",
    "\n",
    "def get_movie_budget_revenue(ids, retries=5, backoff_factor=1):\n",
    "    movie_ids = \" \".join([f\"wd:{id}\" for id in ids])\n",
    "    query = (\n",
    "            'SELECT ?movie ?movieLabel ?budget ?revenue WHERE { '\n",
    "            'VALUES ?movie { ' + movie_ids + ' } '  # Make sure movie_ids is correctly formatted\n",
    "                                             '?movie wdt:P31 wd:Q11424. '\n",
    "                                             'OPTIONAL { ?movie wdt:P2130 ?budget. } '  # Budget property\n",
    "                                             'OPTIONAL { ?movie wdt:P2142 ?revenue. } '  # Revenue property\n",
    "                                             'SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". } '\n",
    "                                             '}'\n",
    "    )\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        query_result = requests.get(WIKI_DATA_SERVICE_URL, params={'format': 'json', 'query': query})\n",
    "\n",
    "        if query_result.status_code == 200:\n",
    "            try:\n",
    "                json_data = query_result.json()\n",
    "                df = pd.DataFrame(json_data['results']['bindings'])\n",
    "                for column in df.columns:\n",
    "                    df[column] = df[column].apply(lambda x: x.get('value') if isinstance(x, dict) else x)\n",
    "                return df\n",
    "            except ValueError as e:\n",
    "                print(\"Error decoding JSON:\", e)\n",
    "                return pd.DataFrame()  # Return an empty DataFrame if JSON error\n",
    "\n",
    "        elif query_result.status_code == 429:\n",
    "            # Exponential backoff\n",
    "            wait_time = backoff_factor * (2 ** attempt)\n",
    "            time.sleep(wait_time)\n",
    "        else:\n",
    "            print(f\"Request failed with status code {query_result.status_code}\")\n",
    "            return pd.DataFrame()  # Return empty DataFrame if other errors\n",
    "\n",
    "    print(\"Max retries reached.\")\n",
    "    return pd.DataFrame()  # Return an empty DataFrame in case of JSON error"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T14:03:31.790158Z",
     "start_time": "2024-12-14T14:03:31.786192Z"
    }
   },
   "id": "e8b4d9ef192c9b55"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows fetched from Wikidata: 65551\n"
     ]
    }
   ],
   "source": [
    "# Filter out NaNs and duplicate IDs for querying\n",
    "unique_ids = book_adaptation_df['movie_wikidata_id'].dropna().unique()\n",
    "\n",
    "# Query Wikidata in batches\n",
    "batch_size = 100\n",
    "movie_df_list = []\n",
    "for i in range(0, len(unique_ids), batch_size):\n",
    "    batch_ids = unique_ids[i:i + batch_size]\n",
    "    fetched_data = get_movie_budget_revenue(pd.Series(batch_ids))\n",
    "    if not fetched_data.empty:\n",
    "        movie_df_list.append(fetched_data)\n",
    "\n",
    "# Concatenate all fetched data into one DataFrame\n",
    "if movie_df_list:\n",
    "    movie_rev_budget = pd.concat(movie_df_list, ignore_index=True)\n",
    "else:\n",
    "    movie_rev_budget = pd.DataFrame(columns=['movie_wikidata_id', 'budget', 'revenue'])\n",
    "\n",
    "# Print count check\n",
    "print(f\"Total rows fetched from Wikidata: {len(movie_rev_budget)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T14:23:02.194058Z",
     "start_time": "2024-12-14T14:03:36.474046Z"
    }
   },
   "id": "52368bbcdbca52c5"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "movie_rev_budget['movie_wikidata_id']=movie_rev_budget['movie']\n",
    "movie_rev_budget=movie_rev_budget.drop(columns=['movie'])\n",
    "movie_rev_budget=movie_rev_budget.drop(columns=['movieLabel'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T14:25:07.094291Z",
     "start_time": "2024-12-14T14:25:07.060669Z"
    }
   },
   "id": "c2fed3cde03b4698"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "movie_rev_budget['movie_wikidata_id'] = movie_rev_budget['movie_wikidata_id'].str.replace('http://www.wikidata.org/entity/', '', regex=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T14:25:07.893704Z",
     "start_time": "2024-12-14T14:25:07.888702Z"
    }
   },
   "id": "4103e9ee5b2426dc"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate movie_wikidata_id entries in movie_rev_budget: 1590\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in movie_rev_budget\n",
    "duplicates_in_movie_rev_budget = movie_rev_budget[movie_rev_budget.duplicated(subset=['movie_wikidata_id'], keep=False)]\n",
    "print(f\"Number of duplicate movie_wikidata_id entries in movie_rev_budget: {len(duplicates_in_movie_rev_budget)}\")\n",
    "# Drop duplicates, keeping the first occurrence\n",
    "movie_rev_budget = movie_rev_budget.drop_duplicates(subset='movie_wikidata_id', keep='first')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T14:25:09.152300Z",
     "start_time": "2024-12-14T14:25:09.126137Z"
    }
   },
   "id": "894ff9819aa47a70"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "movie_rev_budget.to_csv(\"data/book_adaptation_expanded.csv\", index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-14T14:25:12.395073Z",
     "start_time": "2024-12-14T14:25:12.344391Z"
    }
   },
   "id": "5a29d736a83306b6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3c01bf2109d4e645"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
